{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OOtHlgMCaQHn"
      },
      "outputs": [],
      "source": [
        "!pip install natasha"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9edVF6ZH98yc"
      },
      "outputs": [],
      "source": [
        "!pip install lexicalrichness"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U3EETgxMfoqR"
      },
      "outputs": [],
      "source": [
        "!pip install catboost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zyCheE7FaQHo"
      },
      "outputs": [],
      "source": [
        "!pip install -U deep-translator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E9Pvw72naqpz"
      },
      "outputs": [],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1wejvchVXbIU"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from natasha import Segmenter, Doc, NewsEmbedding, NewsMorphTagger, MorphVocab\n",
        "from lexicalrichness import LexicalRichness\n",
        "import numpy as np\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.feature_selection import mutual_info_classif, RFECV, SelectKBest\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "import lightgbm as lgb\n",
        "import xgboost as xgb\n",
        "from catboost import CatBoostClassifier\n",
        "\n",
        "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
        "from deep_translator import GoogleTranslator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CvMXSvbFaQHp"
      },
      "outputs": [],
      "source": [
        "class ParaphraseAugmentation():\n",
        "    def __init__(self):\n",
        "        MODEL_NAME = 'cointegrated/rut5-base-paraphraser'\n",
        "        self.model = T5ForConditionalGeneration.from_pretrained(MODEL_NAME)\n",
        "        self.tokenizer = T5Tokenizer.from_pretrained(MODEL_NAME)\n",
        "        self.model.cuda()\n",
        "        self.model.eval()\n",
        "        self.segmenter = Segmenter()\n",
        "        self.emb = NewsEmbedding()\n",
        "        self.morph_tagger = NewsMorphTagger(self.emb)\n",
        "\n",
        "    def paraphrase(self, text, beams=9, grams=3, do_sample=False):\n",
        "        new_text = ''\n",
        "        doc = Doc(text)\n",
        "        doc.segment(self.segmenter)\n",
        "        doc.tag_morph(self.morph_tagger)\n",
        "        for sentence in doc.sents:\n",
        "            sentence = sentence.text\n",
        "            x = self.tokenizer(sentence, return_tensors='pt', padding=True).to(self.model.device)\n",
        "            max_size = int(x.input_ids.shape[1] * 1.5 + 10)\n",
        "            out = self.model.generate(**x, encoder_no_repeat_ngram_size=grams, num_beams=beams, max_length=max_size, do_sample=do_sample)\n",
        "            pharaphrased =  self.tokenizer.decode(out[0], skip_special_tokens=True)\n",
        "            new_text = new_text + ' ' + pharaphrased\n",
        "        return new_text\n",
        "\n",
        "    def augmentation(self, csv_file_path):\n",
        "        dataframe = pd.read_csv(csv_file_path, sep=';', encoding=\"cp1251\", header=1).dropna().reset_index(drop=True)\n",
        "        count_row = dataframe.shape[0]\n",
        "        for row in range(count_row):\n",
        "            print(row)\n",
        "            dataframe.loc[dataframe.shape[0], 'Text'] = self.paraphrase(dataframe.loc[row, 'Text'], beams=10, grams=7)\n",
        "            dataframe.loc[dataframe.shape[0]-1, 'Style'] = dataframe.loc[row, 'Style']\n",
        "        dataframe.to_csv('texts_augmented (paraphrase).csv', sep=';', encoding='cp1251', errors='ignore', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PvgI5SURaQHp"
      },
      "outputs": [],
      "source": [
        "class TranslateAugmentation():\n",
        "    def __init__(self):\n",
        "        self.translator_en = GoogleTranslator(source='auto', target='en')\n",
        "        self.translator_de = GoogleTranslator(source='auto', target='de')\n",
        "        self.translator_ru = GoogleTranslator(source='auto', target='ru')\n",
        "\n",
        "    def translate(self, text):\n",
        "        translated_en = self.translator_en.translate(text)\n",
        "        translated_de = self.translator_de.translate(translated_en)\n",
        "        translated_ru = self.translator_ru.translate(translated_de)\n",
        "        return translated_ru\n",
        "\n",
        "    def augmentation(self, csv_file_path):\n",
        "        dataframe = pd.read_csv(csv_file_path, sep=';', encoding=\"cp1251\", header=1).dropna().reset_index(drop=True)\n",
        "        count_row = dataframe.shape[0]\n",
        "        for row in range(count_row):\n",
        "            print(row)\n",
        "            try:\n",
        "                new_text = self.translate(dataframe.loc[row, 'Text'])\n",
        "            except Exception as e:\n",
        "                continue\n",
        "            dataframe.loc[dataframe.shape[0], 'Text'] = new_text\n",
        "            dataframe.loc[dataframe.shape[0]-1, 'Style'] = dataframe.loc[row, 'Style']\n",
        "        dataframe.to_csv('texts_augmented (translation).csv', sep=';', encoding='cp1251', errors='ignore', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eFSLwiQaaQHq"
      },
      "outputs": [],
      "source": [
        "translator = TranslateAugmentation()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RxsWhBQ5aQHq"
      },
      "outputs": [],
      "source": [
        "translator.augmentation('texts.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JWJyc3aXaQHq"
      },
      "outputs": [],
      "source": [
        "paraphraser = ParaphraseAugmentation()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eXOiqPnfaQHq"
      },
      "outputs": [],
      "source": [
        "paraphraser.augmentation('texts.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TbYcFPmFpgsm"
      },
      "outputs": [],
      "source": [
        "class FeatureExtractionModule():\n",
        "  def __init__(self):\n",
        "    self.segmenter = Segmenter()\n",
        "    self.emb = NewsEmbedding()\n",
        "    self.morph_tagger = NewsMorphTagger(self.emb)\n",
        "    self.morph_vocab = MorphVocab()\n",
        "\n",
        "  # Преобразование набора данных текстов в набор данных векторов признаков\n",
        "  def fromCsvToCsv(self, csv_file_path):\n",
        "    if(csv_file_path!='texts.csv'):\n",
        "        dataframe = pd.read_csv(csv_file_path, sep=';', encoding=\"cp1251\")\n",
        "    else:\n",
        "        dataframe = pd.read_csv(csv_file_path, sep=';', encoding=\"cp1251\", header=1).dropna().reset_index(drop=True)\n",
        "    dataframe.loc[:, \"Style\"] = dataframe.loc[:, \"Style\"].astype('category').cat.codes # Style from string to int categories\n",
        "    dataframe_rows = dataframe.shape[0]\n",
        "\n",
        "    dataset = pd.DataFrame()\n",
        "    for csv_row in range(dataframe_rows):\n",
        "        text = dataframe.loc[csv_row, \"Text\"]\n",
        "        doc = Doc(text)\n",
        "        doc.segment(self.segmenter)\n",
        "        doc.tag_morph(self.morph_tagger)\n",
        "\n",
        "        dataset.loc[csv_row, 'average_word_length'] = self.averageWordLength(doc)\n",
        "        dataset.loc[csv_row, 'average_word_count'] = self.averageWordCount(doc)\n",
        "        dataset.loc[csv_row, 'log_connection'] = self.logicalConnectionCoefficient(doc)\n",
        "\n",
        "        dataset.loc[csv_row, 'verb_freq'] = self.posFrequencyCoefficient(doc, 'VERB')\n",
        "        dataset.loc[csv_row, 'noun_freq'] = self.posFrequencyCoefficient(doc, 'NOUN')\n",
        "        dataset.loc[csv_row, 'adv_freq'] = self.posFrequencyCoefficient(doc, 'ADV')\n",
        "        dataset.loc[csv_row, 'adj_freq'] = self.posFrequencyCoefficient(doc, 'ADJ')\n",
        "        dataset.loc[csv_row, 'propn_freq'] = self.posFrequencyCoefficient(doc, 'PROPN')\n",
        "\n",
        "        dataset.loc[csv_row, 'verb_noun_freq']  = self.posCombinationFrequencyCoefficient(doc, 'VERB', 'NOUN')\n",
        "        dataset.loc[csv_row, 'verb_adv_freq']  = self.posCombinationFrequencyCoefficient(doc, 'VERB', 'ADV')\n",
        "        dataset.loc[csv_row, 'noun_noun_freq']  = self.posCombinationFrequencyCoefficient(doc, 'NOUN', 'NOUN')\n",
        "        dataset.loc[csv_row, 'adv_noun_freq']  = self.posCombinationFrequencyCoefficient(doc, 'ADJ', 'NOUN')\n",
        "\n",
        "        dataset.loc[csv_row, 'dinamism_static'] = self.dynamismStaticTextCoefficient(doc)\n",
        "\n",
        "        dataset.loc[csv_row, 'dot_freq'] = self.punctFrequencyCoefficient(doc, '.')\n",
        "        dataset.loc[csv_row, 'comma_freq'] = self.punctFrequencyCoefficient(doc, ',')\n",
        "        dataset.loc[csv_row, 'colon_freq'] = self.punctFrequencyCoefficient(doc, ':')\n",
        "        dataset.loc[csv_row, 'semicolon_freq'] = self.punctFrequencyCoefficient(doc, ';')\n",
        "        dataset.loc[csv_row, 'quote_freq'] = self.punctFrequencyCoefficient(doc, '\"')\n",
        "        dataset.loc[csv_row, 'exclamation_freq'] = self.punctFrequencyCoefficient(doc, '!')\n",
        "        dataset.loc[csv_row, 'question_freq'] = self.punctFrequencyCoefficient(doc, '?')\n",
        "        dataset.loc[csv_row, 'dash_freq'] = self.punctFrequencyCoefficient(doc, '—')\n",
        "\n",
        "        dataset.loc[csv_row, 'lex_rich'] = self.lexicalRichnessCoefficient(doc)\n",
        "\n",
        "        dataset.loc[csv_row, 'y (style)'] = dataframe.loc[csv_row, 'Style']\n",
        "    return dataset\n",
        "\n",
        "  # Преобразование 1 текста (для предсказания) в вектор признаков\n",
        "  def fromTextToVector(self, text):\n",
        "        doc = Doc(text)\n",
        "        doc.segment(self.segmenter)\n",
        "        doc.tag_morph(self.morph_tagger)\n",
        "\n",
        "        average_word_length = self.averageWordLength(doc)\n",
        "        average_word_count = self.averageWordCount(doc)\n",
        "        log_connection = self.logicalConnectionCoefficient(doc)\n",
        "\n",
        "        verb_freq = self.posFrequencyCoefficient(doc, 'VERB')\n",
        "        noun_freq = self.posFrequencyCoefficient(doc, 'NOUN')\n",
        "        adv_freq = self.posFrequencyCoefficient(doc, 'ADV')\n",
        "        adj_freq = self.posFrequencyCoefficient(doc, 'ADJ')\n",
        "        propn_freq = self.posFrequencyCoefficient(doc, 'PROPN')\n",
        "\n",
        "        verb_noun_freq  = self.posCombinationFrequencyCoefficient(doc, 'VERB', 'NOUN')\n",
        "        verb_adv_freq = self.posCombinationFrequencyCoefficient(doc, 'VERB', 'ADV')\n",
        "        noun_noun_freq  = self.posCombinationFrequencyCoefficient(doc, 'NOUN', 'NOUN')\n",
        "        adv_noun_freq  = self.posCombinationFrequencyCoefficient(doc, 'ADJ', 'NOUN')\n",
        "\n",
        "        dinamism_static= self.dynamismStaticTextCoefficient(doc)\n",
        "\n",
        "        dot_freq = self.punctFrequencyCoefficient(doc, '.')\n",
        "        comma_freq = self.punctFrequencyCoefficient(doc, ',')\n",
        "        colon_freq = self.punctFrequencyCoefficient(doc, ':')\n",
        "        semicolon_freq = self.punctFrequencyCoefficient(doc, ';')\n",
        "        quote_freq = self.punctFrequencyCoefficient(doc, '\"')\n",
        "        exclamation_freq = self.punctFrequencyCoefficient(doc, '!')\n",
        "        question_freq = self.punctFrequencyCoefficient(doc, '?')\n",
        "        dash_freq = self.punctFrequencyCoefficient(doc, '—')\n",
        "\n",
        "        lex_rich = self.lexicalRichnessCoefficient(doc)\n",
        "\n",
        "        return np.array([average_word_length, average_word_count, log_connection, verb_freq, noun_freq,\n",
        "                         adv_freq, adj_freq, propn_freq, verb_noun_freq, verb_adv_freq, noun_noun_freq,\n",
        "                         adv_noun_freq, dinamism_static, dot_freq, comma_freq, colon_freq, semicolon_freq,\n",
        "                         quote_freq, exclamation_freq, question_freq, dash_freq, lex_rich])\n",
        "\n",
        "  # Подсчёт количества слов в тексте (исключая знаки препинания и цифры и ошибочные слова)\n",
        "  def wordCount(self, doc):\n",
        "    token_count = 0\n",
        "    for token in doc.tokens:\n",
        "      if (token.pos != 'PUNCT' and token.pos != 'X' and token.pos !='NUM'):\n",
        "        token_count = token_count + 1\n",
        "    return token_count\n",
        "\n",
        "  # Подсчёт количества препинаний в тексте\n",
        "  def punctCount(self, doc):\n",
        "    punct_count = 0\n",
        "    for token in doc.tokens:\n",
        "      if (token.pos == 'PUNCT'):\n",
        "        punct_count = punct_count + 1\n",
        "    return punct_count\n",
        "\n",
        "  # Показатель среднего размера токена\n",
        "  def averageWordLength(self, doc):\n",
        "    token_count = self.wordCount(doc)\n",
        "    token_len = []\n",
        "    for token in doc.tokens:\n",
        "      if (token.pos != 'PUNCT' and token.pos != 'X' and token.pos !='NUM'):\n",
        "        token_len.append(len(token.text))\n",
        "    return np.mean(token_len)\n",
        "\n",
        "  # Показатель среднего размера предложения\n",
        "  def averageWordCount(self, doc):\n",
        "    sent_count = len(doc.sents)\n",
        "    token_count = self.wordCount(doc)\n",
        "    return token_count/sent_count\n",
        "\n",
        "  # Коэффицент частотности части речи\n",
        "  def posFrequencyCoefficient(self, doc, pos_tag):\n",
        "    all_token_count = self.wordCount(doc)\n",
        "    pos_count = 0\n",
        "    for token in doc.tokens:\n",
        "      if (token.pos == pos_tag):\n",
        "        pos_count = pos_count + 1\n",
        "    return pos_count/all_token_count\n",
        "\n",
        "  # Коэффицент количества частиречной сочетаемости\n",
        "  def posCombinationFrequencyCoefficient(self, doc, pos_tag_1, pos_tag_2):\n",
        "    all_token_count = self.wordCount(doc)\n",
        "    pos_count = 0\n",
        "    for i in range(1, len(doc.tokens)):\n",
        "      if (doc.tokens[i].pos == pos_tag_1 and doc.tokens[i-1].pos == pos_tag_2):\n",
        "        pos_count = pos_count + 1\n",
        "    return pos_count/(all_token_count-1)\n",
        "\n",
        "  # Частота встречаемости знаков препинания  (точка, запятая, двоеточие, точка с запятой, кавычки, скобки, вопросительный знак и тире)\n",
        "  def punctFrequencyCoefficient(self, doc, punct_type):\n",
        "    all_punct_count = self.punctCount(doc)\n",
        "    punct_count = 0\n",
        "    for token in doc.tokens:\n",
        "      if(token.text == punct_type):\n",
        "        punct_count = punct_count + 1\n",
        "    return punct_count/all_punct_count\n",
        "\n",
        "  # Коэффицент соотношения динамичности и статичности текста.\n",
        "  def dynamismStaticTextCoefficient(self, doc):\n",
        "    verb_noun = self.posCombinationFrequencyCoefficient(doc, 'VERB', 'NOUN')\n",
        "    verb_adv = self.posCombinationFrequencyCoefficient(doc, 'VERB', 'ADV')\n",
        "    noun_noun = self.posCombinationFrequencyCoefficient(doc, 'NOUN', 'NOUN')\n",
        "    adv_noun = self.posCombinationFrequencyCoefficient(doc, 'ADJ', 'NOUN')\n",
        "    if(noun_noun + adv_noun != 0):\n",
        "      return (verb_noun + verb_adv) / (noun_noun + adv_noun)\n",
        "    else:\n",
        "      return 0\n",
        "\n",
        "  # Коэффицент логичной связности\n",
        "  def logicalConnectionCoefficient(self, doc):\n",
        "    all_token_count = self.wordCount(doc)\n",
        "    service_word_count = 0\n",
        "    for token in doc.tokens:\n",
        "      if (token.pos == 'ADP' or  token.pos == 'PART' or token.pos == 'CONJ' or token.pos == 'CCONJ' or token.pos == 'INTJ' or token.pos == 'SCONJ'):\n",
        "        service_word_count = service_word_count + 1\n",
        "    return service_word_count/(3*all_token_count)\n",
        "\n",
        "  # Коэффицент лексического богатсва\n",
        "  def lexicalRichnessCoefficient(self, doc, method='voc-D'):\n",
        "    list_of_tokens = []\n",
        "    for token in doc.tokens:\n",
        "        token.lemmatize(self.morph_vocab)\n",
        "        list_of_tokens.append(token.lemma)\n",
        "    lex = LexicalRichness(list_of_tokens, preprocessor=None, tokenizer=None)\n",
        "    if(method == 'TTR'):\n",
        "      return lex.ttr\n",
        "    if(method == 'RTTR'):\n",
        "      return lex.rttr\n",
        "    if(method == 'CTTR'):\n",
        "      return lex.cttr\n",
        "    if(method == 'MSTTR'):\n",
        "      return lex.msttr(segment_window=25)\n",
        "    if(method == 'MATTR'):\n",
        "      return lex.mattr(window_size=25)\n",
        "    if(method == 'MTLD'):\n",
        "      return lex.mtld(threshold=0.72)\n",
        "    if(method == 'HD-D'):\n",
        "      return lex.hdd(draws=42)\n",
        "    if(method == 'voc-D'):\n",
        "      return lex.vocd(ntokens=40, within_sample=100, iterations=3)\n",
        "    if(method == 'Herdan'):\n",
        "      return lex.Herdan\n",
        "    if(method == 'Summer'):\n",
        "      return lex.Summer\n",
        "    if(method == 'Dugast'):\n",
        "      return lex.Dugast\n",
        "    if(method == 'Maas'):\n",
        "      return lex.Maas\n",
        "    if(method == 'YuleK'):\n",
        "      return lex.yulek\n",
        "    if(method == 'YuleI'):\n",
        "      return lex.yulei\n",
        "    if(method == 'HerdanVm'):\n",
        "      return lex.herdanvm\n",
        "    if(method == 'SimpsonD'):\n",
        "      return lex.simpsond"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s_EggrmNsdbB"
      },
      "outputs": [],
      "source": [
        "feature_extraction = FeatureExtractionModule()\n",
        "dataset = feature_extraction.fromCsvToCsv('texts_augmented (translation).csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3PYb1PT0_uwB"
      },
      "outputs": [],
      "source": [
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AEubb4wFdnfG"
      },
      "outputs": [],
      "source": [
        "X_data = dataset.iloc[:, 0:-1]\n",
        "y_data = dataset.iloc[:, -1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JslxB5RFEcwf"
      },
      "outputs": [],
      "source": [
        "def plot_and_print_mi_scores(X_data, y_data):\n",
        "  mi_scores = mutual_info_classif(X_data, y_data)\n",
        "  mi_scores = pd.Series(mi_scores, name=\"MI Scores\", index=X_data.columns)\n",
        "  mi_scores = mi_scores.sort_values(ascending=True)\n",
        "  ax = mi_scores.plot(kind='barh', figsize=(12, 8), title='Mutual Information Scores', legend=False)\n",
        "  ax.bar_label(ax.containers[0], label_type='edge')\n",
        "  return mi_scores.index[::-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hL4hTbMedqqE"
      },
      "outputs": [],
      "source": [
        "plot_and_print_mi_scores(X_data, y_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BboLrvfmdW6-"
      },
      "outputs": [],
      "source": [
        "scaler = MinMaxScaler()\n",
        "scaler.fit(X_data)\n",
        "X_data.iloc[:, :] = scaler.transform(X_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aosa2EZneyxM"
      },
      "outputs": [],
      "source": [
        "X = X_data.values\n",
        "y = y_data.values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hlllENBbrRa0"
      },
      "outputs": [],
      "source": [
        "class BestEstimatorExtraction():\n",
        "  def __init__(self,\n",
        "               X_data,\n",
        "               y_data):\n",
        "    self.X = X_data\n",
        "    self.y = y_data\n",
        "\n",
        "    # Создание моделей для обучения\n",
        "    self.log_reg_model = LogisticRegression()\n",
        "    self.log_reg_best_parameters_for_all_features = {'C': [50], 'max_iter': [1000], 'penalty': ['l1'], 'random_state': [42], 'solver': ['saga']}\n",
        "    self.log_reg_best_parameters = {'C': [50], 'max_iter': [100], 'penalty': ['l2'], 'random_state': [42], 'solver': ['sag']}\n",
        "    self.log_features = [True, True, True, False, True, True, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True]\n",
        "    self.log_reg_search_parameters = {\"estimator__max_iter\":[100, 500, 1000, 5000],\n",
        "                                      \"estimator__C\":[0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05, 0.1, 0.3, 0.5, 1, 5, 10, 50, 100],\n",
        "                                      \"estimator__penalty\":[\"elasticnet\", \"l1\", \"l2\"],\n",
        "                                      \"estimator__solver\" : ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],\n",
        "                                      \"estimator__random_state\": [42]}\n",
        "\n",
        "    self.dec_tree_model = DecisionTreeClassifier()\n",
        "    self.dec_tree_best_parameters_for_all_features = {'ccp_alpha': [0.01], 'criterion': ['entropy'], 'max_depth': [8], 'max_features': [0.8], 'min_samples_leaf': [1], 'min_samples_split': [2], 'random_state': [42]}\n",
        "    self.dec_tree_best_parameters = {'ccp_alpha': [0.0], 'criterion': ['entropy'], 'max_depth': [8], 'max_features': [0.8], 'min_samples_leaf': [5], 'min_samples_split': [2], 'random_state': [42]}\n",
        "    self.dec_tree_features = [True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True]\n",
        "    self.dec_tree_search_parameters = {'estimator__max_features': ['sqrt', 'log2', 0.2, 0.4, 0.6, 0.8],\n",
        "                                       'estimator__ccp_alpha': [0.1, .01, .001, .0],\n",
        "                                       'estimator__min_samples_leaf': [1, 5, 8, 11],\n",
        "                                       'estimator__min_samples_split': [2, 3, 5, 7, 9],\n",
        "                                       'estimator__max_depth' : [None, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
        "                                       'estimator__criterion' :['gini', 'entropy', 'log_loss'],\n",
        "                                       'estimator__random_state': [42]}\n",
        "\n",
        "    self.random_forest_model = RandomForestClassifier()\n",
        "    self.random_forest_best_parameters_for_all_features = {'ccp_alpha': [0.001], 'criterion': ['entropy'], 'max_depth': [None], 'max_features': ['sqrt'], 'min_samples_leaf': [1], 'min_samples_split': [2], 'n_estimators': [1000], 'random_state': [42]}\n",
        "    self.random_forest_best_parameters = {'ccp_alpha': [0.001], 'criterion': ['entropy'], 'max_depth': [9], 'max_features': ['sqrt'], 'min_samples_leaf': [1], 'min_samples_split': [2], 'n_estimators': [500], 'random_state': [42]}\n",
        "    self.random_forest_features = [True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True]\n",
        "    self.random_forest_search_parameters = {'estimator__n_estimators':[100, 250, 500, 1000],\n",
        "                                            'estimator__max_features': ['sqrt', 'log2', 0.2, 0.4, 0.6, 0.8],\n",
        "                                            'estimator__ccp_alpha': [0.1, .01, .001, 0.0],\n",
        "                                            'estimator__min_samples_leaf': [1, 5, 8, 11],\n",
        "                                            'estimator__min_samples_split': [2, 3, 5, 7, 9],\n",
        "                                            'estimator__max_depth' : [None, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
        "                                            'estimator__criterion' :['gini', 'entropy', 'log_loss'],\n",
        "                                            'estimator__random_state': [42]}\n",
        "\n",
        "    self.svc_model = SVC()\n",
        "    self.svc_best_parameters_for_all_features = {'C': [10], 'gamma': ['scale'], 'kernel': ['rbf'], 'random_state': [42]}\n",
        "    self.svc_best_parameters = {'C': [10], 'gamma': ['scale'], 'kernel': ['linear'], 'random_state': [42]}\n",
        "    self.svc_features = [True, True, True, True, True, True, False, True, True, False, True, True, False, False, True, True, True, True, True, True, True, True]\n",
        "    self.svc_search_parameters = {'estimator__C': [0.1, 1, 10, 100, 1000],\n",
        "                                  'estimator__gamma': ['scale', 'auto', 1, 0.1, 0.01, 0.001, 0.0001],\n",
        "                                  'estimator__kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
        "                                  'estimator__random_state': [42]}\n",
        "\n",
        "    self.knn_model = KNeighborsClassifier()\n",
        "    self.knn_best_parameters = {'n_neighbors': [17], 'p': [1], 'weights': ['distance']}\n",
        "    self.knn_features = [True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True]\n",
        "\n",
        "    self.xgboost_model = xgb.XGBClassifier(objective='multi:softmax', num_class=5, tree_method='hist')\n",
        "    self.xgboost_best_parameters = {'booster': ['gbtree'], 'n_estimators': [500], 'learning_rate': [0.05], 'max_depth': [4], 'subsample': [0.4], 'reg_alpha': [0.1], 'reg_lambda': [1.0], \"random_state\": [42]}\n",
        "    self.xboost_features = [True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True]\n",
        "    self.xgboost_search_parameters = {'estimator__booster' : ['gbtree'],\n",
        "                                      'estimator__n_estimators':[100, 250, 500, 1000],\n",
        "                                      'estimator__learning_rate' : [0.05, 0.1, 0.2, 0.3],\n",
        "                                      'estimator__max_depth' : [None, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
        "                                      'estimator__subsample' : [0.4, 0.5, 0.6, 0.7, 0.8, 0.9],\n",
        "                                      'estimator__reg_alpha' : [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1],\n",
        "                                      'estimator__reg_lambda' : [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1],\n",
        "                                      'estimator__random_state': [42]\n",
        "                                      }\n",
        "\n",
        "    self.catboost_model = CatBoostClassifier(verbose=False)\n",
        "    self.catboost_best_parameters = {'n_estimators':[1000], 'learning_rate' : [0.2], 'max_depth' : [7], 'l2_leaf_reg':[3], 'random_strength': [0.2],'bagging_temperature':[1.0], 'border_count':[254], 'random_state': [42]}\n",
        "    self.catboost_features = [True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True]\n",
        "    self.catboost_search_parameters = {'n_estimators':[100, 250, 500, 1000],\n",
        "                                      'learning_rate' : [0.05, 0.1, 0.2, 0.3],\n",
        "                                      'max_depth' : [None, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
        "                                      'l2_leaf_reg':[1, 3, 5, 10, 100],\n",
        "                                      'random_strength': [0.2, 0.5, 0.8, 1.1, 1.4],\n",
        "                                      'bagging_temperature':[0.03, 0.09, 0.25, 0.75, 1.0],\n",
        "                                      'border_count':[254],\n",
        "                                      'random_state': [42]}\n",
        "\n",
        "    self.lgbm_model = lgb.LGBMClassifier(objective='multiclass', n_jobs=-1)\n",
        "    self.lgbm_best_parameters = {'n_estimators': [250],'max_depth': [None],'learning_rate' : [0.3],'subsample' : [0.4],'reg_alpha' : [0.1],'reg_lambda' : [0.1],'num_leaves': [32],'is_unbalance': [False],'boost_from_average': [False], \"random_state\": [42]}\n",
        "    self.lgbm_features = [True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True]\n",
        "    self.lgbm_search_parameters = {'estimator__n_estimators': [100, 250, 500, 1000],\n",
        "                                   'estimator__max_depth': [None, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
        "                                   'estimator__learning_rate' : [0.05, 0.1, 0.2, 0.3],\n",
        "                                   'estimator__subsample' : [0.4, 0.5, 0.6, 0.7, 0.8, 0.9],\n",
        "                                   'estimator__reg_alpha' : [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1],\n",
        "                                   'estimator__reg_lambda' : [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1],\n",
        "                                   'estimator__num_leaves': [32],\n",
        "                                   'estimator__is_unbalance': [False],\n",
        "                                   'estimator__boost_from_average': [False],\n",
        "                                   \"estimator__random_state\": [42]}\n",
        "\n",
        "  # Методы для поиска оптимального количества признаков и параметров RFECV + GridSearch\n",
        "  def modelGridSearch(self, model, parameters):\n",
        "    cv_method = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "    rfecv = RFECV(estimator=model, step=1, cv=cv_method, scoring='accuracy')\n",
        "    clf = GridSearchCV(rfecv, parameters, scoring = 'accuracy', cv=cv_method, verbose=0)\n",
        "    clf_results = clf.fit(self.X, self.y)\n",
        "    return clf_results\n",
        "\n",
        "  def logisticRegressionGridSearch(self):\n",
        "      return self.modelGridSearch(self.log_reg_model, self.log_reg_search_parameters)\n",
        "\n",
        "  def decisionTreeGridSearch(self):\n",
        "      return self.modelGridSearch(self.dec_tree_model, self.dec_tree_search_parameters)\n",
        "\n",
        "  def randomForestGridSearch(self):\n",
        "      return self.modelGridSearch(self.random_forest_model, self.random_forest_search_parameters)\n",
        "\n",
        "  def svcGridSearch(self):\n",
        "      return self.modelGridSearch(self.svc_model, self.svc_search_parameters)\n",
        "\n",
        "  def xgboostGridSearch(self):\n",
        "      return self.modelGridSearch(self.xgboost_model, self.xgboost_search_parameters)\n",
        "\n",
        "  def catBoostGridSearch(self):\n",
        "      return self.modelGridSearch(self.catboost_model, self.catboost_search_parameters)\n",
        "\n",
        "  def lgbmGridSearch(self):\n",
        "      return self.modelGridSearch(self.lgbm_model, self.lgbm_search_parameters)\n",
        "\n",
        "  def knnGridSearch(self):\n",
        "      pipe = Pipeline([('selector', SelectKBest(mutual_info_classif, k=5)),\n",
        "                     ('estimator', KNeighborsClassifier())])\n",
        "      parameters = {'selector__k': np.arange(1,23),\n",
        "                    'estimator__n_neighbors': np.arange(1,50),\n",
        "                    'estimator__weights' : ['uniform', 'distance'],\n",
        "                    'estimator__p' : [1, 2]}\n",
        "      cv_method = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "      clf = GridSearchCV(pipe, parameters, scoring = 'accuracy', cv=cv_method, verbose=0)\n",
        "      clf_results = clf.fit(X, y)\n",
        "      return clf_results\n",
        "\n",
        "  # Методы для обучения с уже найденными оптимальным количеством признаков и параметрами\n",
        "  def modelFit(self, model, parameters, features):\n",
        "    cv_method = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "    clf = GridSearchCV(model, parameters, scoring = 'accuracy', cv=cv_method, verbose=0)\n",
        "    clf_results = clf.fit(self.X[:, features], self.y)\n",
        "    print('Best accuracy score of : ' + model.__class__.__name__ + \" \" + str(clf_results.best_score_))\n",
        "    return clf_results.best_estimator_, features\n",
        "\n",
        "  def logisticRegressionFit(self):\n",
        "      return self.modelFit(self.log_reg_model, self.log_reg_best_parameters, self.log_features)\n",
        "\n",
        "  def decisionTreeFit(self):\n",
        "      return self.modelFit(self.dec_tree_model, self.dec_tree_best_parameters, self.dec_tree_features)\n",
        "\n",
        "  def randomForestFit(self):\n",
        "      return self.modelFit(self.random_forest_model, self.random_forest_best_parameters, self.random_forest_features)\n",
        "\n",
        "  def svcFit(self):\n",
        "      return self.modelFit(self.svc_model, self.svc_best_parameters, self.svc_features)\n",
        "\n",
        "  def knnFit(self):\n",
        "      return self.modelFit(self.knn_model, self.knn_best_parameters, self.knn_features)\n",
        "\n",
        "  def xgboostFit(self):\n",
        "      return self.modelFit(self.xgboost_model, self.xgboost_best_parameters, self.xboost_features)\n",
        "\n",
        "  def catBoostFit(self):\n",
        "      return self.modelFit(self.catboost_model, self.catboost_best_parameters, self.catboost_features)\n",
        "\n",
        "  def lgbmFit(self):\n",
        "      return self.modelFit(self.lgbm_model, self.lgbm_best_parameters, self.lgbm_features)\n",
        "\n",
        "  def modelPredict(self, model, text, feature_extraction, scaler, features):\n",
        "    text_features = feature_extraction.fromTextToVector(text)[None, :]\n",
        "    text_features = scaler.transform(text_features)\n",
        "    text_features = text_features[:, features]\n",
        "    proba = model.predict_proba(text_features)\n",
        "    return proba"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X_QYvj5OaQHt"
      },
      "outputs": [],
      "source": [
        "bee = BestEstimatorExtraction(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QGOh4w5paQHt",
        "outputId": "e5ecbb4b-a118-4990-a700-8828094e82b9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best accuracy score of : LogisticRegression 0.9016082920870014\n",
            "Best accuracy score of : DecisionTreeClassifier 0.8318232767849271\n",
            "Best accuracy score of : RandomForestClassifier 0.9156610118260474\n",
            "Best accuracy score of : SVC 0.9041195954710897\n",
            "Best accuracy score of : KNeighborsClassifier 0.9036044886084558\n",
            "Best accuracy score of : XGBClassifier 0.9241961688139948\n",
            "Best accuracy score of : CatBoostClassifier 0.9342350852004383\n",
            "Best accuracy score of : LGBMClassifier 0.9292149972922255\n"
          ]
        }
      ],
      "source": [
        "log_reg, log_reg_features = bee.logisticRegressionFit()\n",
        "dec_tree, dec_tree_features  = bee.decisionTreeFit()\n",
        "random_forest, random_forest_features = bee.randomForestFit()\n",
        "svc, svc_features = bee.svcFit()\n",
        "knn, knn_features = bee.knnFit()\n",
        "xgboost, xgboost_features = bee.xgboostFit()\n",
        "catboost, catboost_features = bee.catBoostFit()\n",
        "lgbm, lgbm_features = bee.lgbmFit()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_fgkIE61H3aV"
      },
      "source": [
        "Saving models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oAAhez8PCFMV"
      },
      "outputs": [],
      "source": [
        "import joblib\n",
        "\n",
        "joblib.dump(log_reg, 'log_reg.pkl')\n",
        "joblib.dump(dec_tree, 'dec_tree.pkl')\n",
        "joblib.dump(random_forest, 'random_forest.pkl')\n",
        "joblib.dump(svc, 'svc.pkl')\n",
        "joblib.dump(knn, 'knn.pkl')\n",
        "joblib.dump(xgboost, 'xgboost.pkl')\n",
        "joblib.dump(catboost, 'catboost.pkl')\n",
        "joblib.dump(lgbm, 'lgbm.pkl')\n",
        "\n",
        "joblib.dump(scaler, 'scaler.pkl')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9xx6NTV7lcoW"
      },
      "source": [
        "###**Neural Network**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YLg9CBxQjUw3"
      },
      "outputs": [],
      "source": [
        "!pip install lightning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dbWtOCB2qoak"
      },
      "outputs": [],
      "source": [
        "!pip install optuna"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cAsr3q1Nf-aJ"
      },
      "outputs": [],
      "source": [
        "import lightning.pytorch as pl\n",
        "from torch.utils.data import TensorDataset, random_split, DataLoader\n",
        "from torch import nn\n",
        "import torch\n",
        "import optuna\n",
        "from optuna.integration import PyTorchLightningPruningCallback\n",
        "from lightning.pytorch.callbacks.early_stopping import EarlyStopping\n",
        "import torchmetrics\n",
        "from lightning.pytorch.callbacks import Callback, ModelCheckpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1BUxVu2IpH4d"
      },
      "outputs": [],
      "source": [
        "feature_extraction = FeatureExtractionModule()\n",
        "dataset = feature_extraction.fromCsvToCsv('texts_augmented (translation).csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ydIcA8AfqQy-"
      },
      "outputs": [],
      "source": [
        "X_data = dataset.iloc[:, 0:-1]\n",
        "y_data = dataset.iloc[:, -1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-JEu_F_voYsV"
      },
      "outputs": [],
      "source": [
        "# mutual information\n",
        "mi_scores = mutual_info_classif(X_data, y_data)\n",
        "mi_scores = pd.Series(mi_scores, name=\"MI Scores\", index=X_data.columns)\n",
        "mi_scores = mi_scores.sort_values(ascending=True)\n",
        "feature_important_list = mi_scores.index[::-1]\n",
        "features_indexes = [list(X_data.columns).index(feature_important_list[i]) for i in range(len(feature_important_list))]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GvBar74BqHt9"
      },
      "outputs": [],
      "source": [
        "X_data = X_data.reindex(feature_important_list, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rKeK1qGXo5s_"
      },
      "outputs": [],
      "source": [
        "scaler = MinMaxScaler()\n",
        "scaler.fit(X_data)\n",
        "X_data.iloc[:, :] = scaler.transform(X_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4hmd4z6Eo9EQ"
      },
      "outputs": [],
      "source": [
        "X = X_data.values\n",
        "y = y_data.values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QfqNynQd1UOe"
      },
      "outputs": [],
      "source": [
        "class MLPSearch(nn.Module):\n",
        "    def __init__(self,\n",
        "                 in_features,\n",
        "                 num_classes,\n",
        "                 n_layers_out_features,\n",
        "                 dropout):\n",
        "        super().__init__()\n",
        "        layers = []\n",
        "\n",
        "        input_dim = in_features\n",
        "        for output_dim in n_layers_out_features:\n",
        "            layers.append(nn.Linear(input_dim, output_dim))\n",
        "            layers.append(nn.ReLU())\n",
        "            layers.append(nn.Dropout(dropout))\n",
        "            input_dim = output_dim\n",
        "\n",
        "        layers.append(nn.Linear(input_dim, num_classes))\n",
        "\n",
        "        self.layers = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A6qSCHUE2p9A"
      },
      "outputs": [],
      "source": [
        "class ModelCompilation(pl.LightningModule):\n",
        "    def __init__(self,\n",
        "                 model:torch.nn.Module,\n",
        "                 metrics:dict,\n",
        "                 loss_function,\n",
        "                 optimizer:torch.optim,\n",
        "                 learning_rate:float):\n",
        "        super().__init__()\n",
        "        self.model = model\n",
        "        self.metrics = metrics\n",
        "        self.loss_function = loss_function\n",
        "        self.optimizer = optimizer\n",
        "        self.learning_rate = learning_rate\n",
        "        self.save_hyperparameters(logger=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        pred = self.model.forward(x)\n",
        "        return pred\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        train_optimizer = self.optimizer(self.parameters(), lr=self.learning_rate)\n",
        "        return train_optimizer\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        loss, pred, y = self.common_step(batch, batch_idx, 'train')\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        loss, pred, y = self.common_step(batch, batch_idx, 'val')\n",
        "        return loss\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        loss, pred, y = self.common_step(batch, batch_idx, 'test')\n",
        "        return loss\n",
        "\n",
        "    def common_step(self, batch, batch_idx, stage):\n",
        "        x, y = batch\n",
        "        pred = self.forward(x)\n",
        "        loss = self.loss_function(pred, y)\n",
        "        if (stage == 'test') or (stage == 'val'):\n",
        "            on_step = False\n",
        "        else:\n",
        "            on_step = True\n",
        "\n",
        "        [self.log(stage + '_' + metric_name, metric(pred, y), on_step=on_step, on_epoch=True, prog_bar=True, logger=True) for metric_name, metric in self.metrics.items()]\n",
        "        self.log(stage + '_' + 'loss', loss, on_step=on_step, on_epoch=True, prog_bar=True, logger=True)\n",
        "        return loss, pred, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4TTuFJRC3Amo"
      },
      "outputs": [],
      "source": [
        "def objective(trial: optuna.trial.Trial):\n",
        "  num_classes = 5\n",
        "  task = 'multiclass'\n",
        "  batch_size = 128\n",
        "  metrics = {'accuracy': torchmetrics.Accuracy(task=task, num_classes=num_classes)}\n",
        "  loss_function = nn.CrossEntropyLoss()\n",
        "  optimizer = torch.optim.Adam\n",
        "\n",
        "  in_features = trial.suggest_int(\"n_best_features\", 1, 22)\n",
        "  learning_rate = trial.suggest_float(\"learning_rate\", 1e-5, 1e-2)\n",
        "  n_layers = trial.suggest_int(\"n_layers\", 1, 4)\n",
        "  dropout = trial.suggest_float(\"dropout\", 0.1, 0.7)\n",
        "  output_dims = [\n",
        "      trial.suggest_int(\"n_units_l_{}\".format(i), 64, 512, log=True) for i in range(n_layers)\n",
        "  ]\n",
        "\n",
        "  tensor_dataset = TensorDataset(torch.tensor(X[:, 0:in_features]).float(), torch.tensor(y).long())\n",
        "\n",
        "  n_splits = 5\n",
        "  cv_method = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "  kfold_losses = []\n",
        "\n",
        "  for fold, (train_ids, test_ids) in enumerate(cv_method.split(dataset)):\n",
        "\n",
        "    train_data = torch.utils.data.Subset(tensor_dataset, train_ids)\n",
        "    val_data = torch.utils.data.Subset(tensor_dataset, test_ids)\n",
        "    train_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "    val_dataloader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    network = MLPSearch(in_features, num_classes, output_dims, dropout)\n",
        "    model = ModelCompilation(network, metrics, loss_function, optimizer, learning_rate)\n",
        "\n",
        "    checkpoint_callback = ModelCheckpoint(monitor='val_loss', filename='model-{epoch:02d}-{val_loss:.2f}-{val_accuracy:.2f}')\n",
        "    early_stopping_callback = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=10)\n",
        "\n",
        "    trainer = pl.Trainer(callbacks=[early_stopping_callback, checkpoint_callback], precision='32', accelerator=\"cpu\", devices=\"auto\", max_epochs=150, enable_model_summary=False, enable_progress_bar=False)\n",
        "    trainer.fit(model, train_dataloader, val_dataloader)\n",
        "\n",
        "    kfold_losses.append(checkpoint_callback.best_model_score)\n",
        "\n",
        "  return torch.tensor(np.mean(kfold_losses))\n",
        "\n",
        "# Подбор гиперпараметров осуществляется TPESampler Tree-structured Parzen Estimator\n",
        "def searchHyperparameters():\n",
        "  pruner = optuna.pruners.ThresholdPruner(lower=0.03)\n",
        "  study = optuna.create_study(direction=\"minimize\", pruner=pruner)\n",
        "  study.optimize(objective, n_trials=300, gc_after_trial=True, timeout=None)\n",
        "  trial = study.best_trial\n",
        "  print(\"Number of finished trials: {}\".format(len(study.trials)))\n",
        "  print(\"Best trial:\")\n",
        "  print(\"  Value: {}\".format(trial.value))\n",
        "  print(\"  Params: \")\n",
        "  for key, value in trial.params.items():\n",
        "    print(\"    {}: {}\".format(key, value))\n",
        "  return trial.params"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_params = searchHyperparameters()"
      ],
      "metadata": {
        "id": "96t4rpjOIgZx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yj_bE6qNiE7a"
      },
      "outputs": [],
      "source": [
        "def load_pretrained_model(model_params, check_point=None):\n",
        "  num_classes = 5\n",
        "  task = 'multiclass'\n",
        "  metrics = {'accuracy': torchmetrics.Accuracy(task=task, num_classes=num_classes)}\n",
        "  loss_function = nn.CrossEntropyLoss()\n",
        "  optimizer = torch.optim.Adam\n",
        "\n",
        "  num_classes = 5\n",
        "  in_features = model_params['n_best_features']\n",
        "  learning_rate = model_params['learning_rate']\n",
        "  dropout = model_params['dropout']\n",
        "  output_dims = []\n",
        "  for key, value in model_params.items():\n",
        "    if('n_units_l' in key):\n",
        "      output_dims.append(value)\n",
        "\n",
        "  network = MLPSearch(in_features, num_classes, output_dims, dropout)\n",
        "  model = ModelCompilation(network, metrics, loss_function, optimizer, learning_rate)\n",
        "  if(check_point):\n",
        "    model = model.load_from_checkpoint(check_point)\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gz5_y4T5Wq-D"
      },
      "outputs": [],
      "source": [
        "def finalFit(model_params):\n",
        "  model = load_pretrained_model(model_params)\n",
        "\n",
        "  in_features = model_params['n_best_features']\n",
        "  batch_size = 32\n",
        "  tensor_dataset = TensorDataset(torch.tensor(X[:, features_indexes[0:in_features]]).float(), torch.tensor(y).long())\n",
        "  train_size = int(0.8 * len(tensor_dataset))\n",
        "  val_size = len(tensor_dataset) - train_size\n",
        "  train_data, val_data = random_split(tensor_dataset, [train_size, val_size])\n",
        "\n",
        "  train_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "  val_dataloader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "  checkpoint_callback = ModelCheckpoint(monitor='val_loss', filename='model-{epoch:02d}-{val_loss:.2f}-{val_accuracy:.2f}')\n",
        "  early_stopping_callback = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=20)\n",
        "\n",
        "  trainer = pl.Trainer(callbacks=[early_stopping_callback, checkpoint_callback], precision='32', accelerator=\"cpu\", devices=\"auto\", max_epochs=150)\n",
        "  trainer.fit(model, train_dataloader, val_dataloader)\n",
        "  best_model = load_pretrained_model(model_params, checkpoint_callback.best_model_path)\n",
        "  return best_model, checkpoint_callback.best_model_path"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "network, checkpoint = finalFit(model_params)"
      ],
      "metadata": {
        "id": "pyqgJ3iRAY6t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "rstPc8VyGTWm",
        "outputId": "a28145cd-80ae-49d4-ed37-51968aeeae48"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/lightning_logs/version_1577/checkpoints/model-epoch=40-val_loss=0.11-val_accuracy=0.95.ckpt'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NwRf_vQ6jYoT"
      },
      "source": [
        "Saving MLP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-QpdDdrgimzO"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "\n",
        "files.download(checkpoint)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zSP3GPLKiRRR"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "with open('mlp_parameters.pkl', 'wb') as file:\n",
        "    pickle.dump(model_params, file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LDjDc1RijhMy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f02f61d4-3e4c-4194-9c62-85d0a412cdbc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['mlp_scaler.pkl']"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ],
      "source": [
        "import joblib\n",
        "\n",
        "joblib.dump(scaler, 'mlp_scaler.pkl')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}