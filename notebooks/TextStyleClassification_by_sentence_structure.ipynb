{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install lightning"
      ],
      "metadata": {
        "id": "04_-BSSMjip3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install natasha"
      ],
      "metadata": {
        "id": "HN5V1Gd2DoJt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torchtext\n",
        "import torch\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from collections import Counter\n",
        "import natasha\n",
        "from natasha import Segmenter, Doc\n",
        "from torch.utils.data import TensorDataset, random_split, DataLoader\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "import torchmetrics\n",
        "import lightning.pytorch as pl\n",
        "from lightning.pytorch.callbacks.early_stopping import EarlyStopping\n",
        "from lightning.pytorch.callbacks import ModelCheckpoint\n",
        "from pytorch_lightning.callbacks import ModelSummary\n",
        "from lightning.pytorch.tuner import Tuner\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "from lightning.pytorch.callbacks import Callback\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "0xfRNNiFfX75",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "80276426-a1f9-4863-fe53-a7c8de0e2e96"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-2f3a43527ef3>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbuild_vocab_from_iterator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mnatasha\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnatasha\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSegmenter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDoc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTensorDataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_split\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'natasha'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Text classification with (CharCNN + TokenCNN) or (CharCNN + TokenCNNRNN)**"
      ],
      "metadata": {
        "id": "w_cUarOOk8tv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "csv_file_path = '/content/texts_augmented (translation).csv'\n",
        "dataframe = pd.read_csv(csv_file_path, sep=';', encoding=\"cp1251\")\n",
        "dataframe.loc[:, \"Style\"] = dataframe.loc[:, \"Style\"].astype('category').cat.codes"
      ],
      "metadata": {
        "id": "T_qy6s9ja8mY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_text = dataframe.iloc[:, 0].values\n",
        "y_targets = dataframe.iloc[:, 1].values"
      ],
      "metadata": {
        "id": "zLOx_H_CiCFX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Предобработка текста для словаря\n",
        "X_text = [text.lower() for text in X_text]\n",
        "all_train_texts = [' '.join(text for text in X_text)][0]\n",
        "counts = Counter(all_train_texts)\n",
        "vocab = sorted(counts, key=counts.get, reverse=True)\n",
        "vocab_to_int = {word: i for i, word in enumerate(counts, 1)}\n",
        "vocab_to_int['PAD'] = 0\n",
        "vocab_size = len(vocab_to_int.keys())"
      ],
      "metadata": {
        "id": "pfwszH5QCZjf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_to_int"
      ],
      "metadata": {
        "id": "yJNxNEG6prtk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Нахождение максимальной длины слова\n",
        "def maxTokenLen(X_text):\n",
        "  segmenter = Segmenter()\n",
        "  token_lens = []\n",
        "  for text in X_text:\n",
        "    doc = Doc(text)\n",
        "    doc.segment(segmenter)\n",
        "    token_lens.append(max(len(token.text) for token in doc.tokens))\n",
        "  max_token_len = max(token_lens)\n",
        "  return max_token_len\n",
        "\n",
        "# Нахождение максимальной длины текста (в словах)\n",
        "def maxTextLen(X_text):\n",
        "  segmenter = Segmenter()\n",
        "  text_lens = []\n",
        "  for text in X_text:\n",
        "    doc = Doc(text)\n",
        "    doc.segment(segmenter)\n",
        "    text_tokens = [token.text for token in doc.tokens]\n",
        "    text_lens.append(len(text_tokens))\n",
        "  max_text_len = max(text_lens)\n",
        "  return max_text_len\n",
        "\n",
        "# Преобразование текста в тензор\n",
        "def textToTensorDataset(X_data, y_data, vocab_to_int, max_text_len, max_token_len):\n",
        "  segmenter = Segmenter()\n",
        "  num_texts = len(X_data)\n",
        "  texts_tensor = torch.zeros(num_texts, max_text_len, max_token_len + 2).long()\n",
        "  targets_tensor = torch.tensor(y_data).long()\n",
        "\n",
        "  for text_i, text in enumerate(X_text):\n",
        "    doc = Doc(text)\n",
        "    doc.segment(segmenter)\n",
        "    text_tokens = [token.text for token in doc.tokens]\n",
        "    for token_i, token in enumerate(text_tokens):\n",
        "      for char_i, char in enumerate(token):\n",
        "        texts_tensor[text_i, token_i, char_i + 1] = vocab_to_int[char]\n",
        "  return TensorDataset(texts_tensor, targets_tensor)"
      ],
      "metadata": {
        "id": "QpTPb_5boEbt"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_text_len = maxTextLen(X_text)\n",
        "max_token_len = maxTokenLen(X_text)\n",
        "train_dataset = textToTensorDataset(X_text, y_targets, vocab_to_int, max_text_len, max_token_len)"
      ],
      "metadata": {
        "id": "6Ax_Gpk1IR5b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Сверточный слой с функцией активации LeakyRelu\n",
        "class CustomConv1D(nn.Module):\n",
        "  def __init__(self,\n",
        "               in_channels:int,\n",
        "               out_channels:int,\n",
        "               kernel_size:int,\n",
        "               dilation:int):\n",
        "    super().__init__()\n",
        "    self.conv1d = nn.Conv1d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=1, padding=kernel_size//2, dilation=dilation)\n",
        "    self.activation = nn.LeakyReLU(0.2);\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.conv1d(x)\n",
        "    x = self.activation(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "e1Ud1iKg7pLC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Слой для преобразования каждого символа в вектор\n",
        "class CharCNN(nn.Module):\n",
        "  def __init__(self,\n",
        "               char_embed_size:int,\n",
        "               layers_n:int,\n",
        "               kernel_size:int,\n",
        "               dilation:int):\n",
        "    super().__init__()\n",
        "    self.block_list = nn.ModuleList([CustomConv1D(in_channels=char_embed_size, out_channels=char_embed_size, kernel_size=kernel_size, dilation=dilation) for i in range(layers_n)])\n",
        "\n",
        "  def forward(self, x):\n",
        "    for block in self.block_list:\n",
        "      x = x + block(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "NkFjhla3AE46"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Завершающий линейный слой с дропаутом\n",
        "class ClassificationHead(nn.Module):\n",
        "  def __init__(self,\n",
        "               in_features:int,\n",
        "               out_features:int,\n",
        "               dropout:float):\n",
        "    super().__init__()\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    self.classifier = nn.Linear(in_features=in_features, out_features=out_features)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.dropout(x)\n",
        "    x = self.classifier(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "MkI2ouQvF-fp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Преобразование каждого токена (слова) в вектор\n",
        "# с помощью нескольких сверточных слоев и пулинга\n",
        "class TokenCNN(nn.Module):\n",
        "  def __init__(self,\n",
        "               in_embed_size:int,\n",
        "               context_embed_size:int):\n",
        "    super().__init__()\n",
        "    self.conv_1_1 = CustomConv1D(in_channels=in_embed_size, out_channels=context_embed_size, kernel_size=7, dilation=1)\n",
        "    self.conv_1_2 = CustomConv1D(in_channels=context_embed_size, out_channels=context_embed_size, kernel_size=3, dilation=1)\n",
        "    self.pooling_1 = nn.MaxPool1d(kernel_size=3, stride=3)\n",
        "\n",
        "    self.conv_2_1 = CustomConv1D(in_channels=context_embed_size, out_channels=context_embed_size, kernel_size=7, dilation=1)\n",
        "    self.conv_2_2 = CustomConv1D(in_channels=context_embed_size, out_channels=context_embed_size, kernel_size=3, dilation=1)\n",
        "    self.pooling_2 = nn.MaxPool1d(kernel_size=3, stride=3)\n",
        "\n",
        "    self.conv_3_1 = CustomConv1D(in_channels=context_embed_size, out_channels=context_embed_size, kernel_size=7, dilation=1)\n",
        "    self.conv_3_2 = CustomConv1D(in_channels=context_embed_size, out_channels=context_embed_size, kernel_size=3, dilation=1)\n",
        "    self.pooling_3 = nn.MaxPool1d(kernel_size=3, stride=3)\n",
        "\n",
        "    self.conv_4_1 = CustomConv1D(in_channels=context_embed_size, out_channels=context_embed_size, kernel_size=7, dilation=1)\n",
        "    self.conv_4_2 = CustomConv1D(in_channels=context_embed_size, out_channels=context_embed_size, kernel_size=3, dilation=1)\n",
        "    self.pooling_4 = nn.MaxPool1d(kernel_size=3, stride=3)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.conv_1_1(x)\n",
        "    x = x + self.conv_1_2(x)\n",
        "    x = self.pooling_1(x)\n",
        "\n",
        "    x = self.conv_2_1(x)\n",
        "    x = x + self.conv_2_2(x)\n",
        "    x = self.pooling_2(x)\n",
        "\n",
        "    x = self.conv_3_1(x)\n",
        "    x = x + self.conv_3_2(x)\n",
        "    x = self.pooling_3(x)\n",
        "\n",
        "    x = self.conv_4_1(x)\n",
        "    x = x + self.conv_4_2(x)\n",
        "    x = self.pooling_4(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "fhBVeuAveCig",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "fe891c97-0fb2-4fa4-d550-89ce6282e1e5"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-4eba09f27fd4>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Преобразование каждого токена (слова) в вектор\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mTokenCNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m   def __init__(self,\n\u001b[1;32m      4\u001b[0m                \u001b[0min_embed_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                context_embed_size:int):\n",
            "\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Преобразование каждого токена (слова) в вектор\n",
        "# с помощью свертки и lstm\n",
        "class TokenCNNRNN(nn.Module):\n",
        "  def __init__(self,\n",
        "               max_text_len:int,\n",
        "               in_embed_size:int,\n",
        "               context_embed_size:int,\n",
        "               num_layers:int):\n",
        "    super().__init__()\n",
        "    self.num_layers = num_layers\n",
        "    self.context_embed_size = context_embed_size\n",
        "\n",
        "    self.conv_1_1 = CustomConv1D(in_channels=in_embed_size, out_channels=context_embed_size, kernel_size=7, dilation=1)\n",
        "    self.conv_1_2 = CustomConv1D(in_channels=context_embed_size, out_channels=context_embed_size, kernel_size=3, dilation=1)\n",
        "    self.pooling_1 = nn.MaxPool1d(kernel_size=3, stride=3)\n",
        "\n",
        "    self.ltsm = nn.LSTM(max_text_len//3,\n",
        "                       context_embed_size,\n",
        "                       num_layers = num_layers,\n",
        "                       bidirectional = True,\n",
        "                       dropout = 0.1,\n",
        "                       batch_first = True\n",
        "                      )\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.conv_1_1(x)\n",
        "    x = x + self.conv_1_2(x)\n",
        "    x = self.pooling_1(x)\n",
        "\n",
        "    h0 = torch.zeros(self.num_layers*2, x.size(0), self.context_embed_size).to(device)\n",
        "    c0 = torch.zeros(self.num_layers*2, x.size(0), self.context_embed_size).to(device)\n",
        "    out, (hidden_state, cell_state) = self.ltsm(x, (h0, c0))\n",
        "    return out"
      ],
      "metadata": {
        "id": "VnWJUcJZ4kF_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Реализация сети\n",
        "class Network(nn.Module):\n",
        "    def __init__(self,\n",
        "                 cnn_rnn,\n",
        "                 vocab_size,\n",
        "                 num_classes,\n",
        "                 max_text_len,\n",
        "                 char_embedding_size=64,\n",
        "                 token_embedding_size=256,\n",
        "                 classifier_dropout=0.5):\n",
        "        super().__init__()\n",
        "        self.cnn_rnn = cnn_rnn\n",
        "        self.char_embedding_size = char_embedding_size\n",
        "        self.char_embeddings = nn.Embedding(vocab_size, char_embedding_size, padding_idx=0)\n",
        "        self.char_cnn = CharCNN(char_embedding_size, layers_n=10, kernel_size=3, dilation=1)\n",
        "        self.global_pooling_chars = nn.AdaptiveMaxPool1d(1)\n",
        "        self.token_cnn = TokenCNN(in_embed_size=char_embedding_size, context_embed_size=token_embedding_size)\n",
        "        self.token_rnn = TokenCNNRNN(max_text_len=max_text_len, in_embed_size=char_embedding_size, context_embed_size=token_embedding_size, num_layers=1)\n",
        "        self.global_pooling_context = nn.AdaptiveMaxPool1d(1)\n",
        "        self.classification_head = ClassificationHead(in_features=token_embedding_size, out_features=num_classes, dropout=classifier_dropout)\n",
        "\n",
        "    def forward(self, tokens):\n",
        "        batch_size, max_text_len, max_token_len = tokens.shape              # BatchSize x MaxTextLen x MaxTokenLen\n",
        "        tokens_flat = tokens.view(batch_size * max_text_len, max_token_len) # BatchSize*MaxTextLen x MaxTokenLen\n",
        "\n",
        "        char_embeddings = self.char_embeddings(tokens_flat)  # BatchSize*MaxTextLen x MaxTokenLen x EmbSize\n",
        "        char_embeddings = char_embeddings.permute(0, 2, 1)   # BatchSize*MaxTextLen x EmbSize x MaxTokenLen\n",
        "        char_features = self.char_cnn(char_embeddings)       # BatchSize*MaxTextLen x EmbSize x MaxTokenLen\n",
        "\n",
        "        token_features = self.global_pooling_chars(char_features).squeeze(-1)                     # BatchSize*MaxTextLen x EmbSize\n",
        "        token_features = token_features.view(batch_size, max_text_len, self.char_embedding_size)  # BatchSize x MaxTextLen x EmbSize\n",
        "        token_features = token_features.permute(0, 2, 1)                                          # BatchSize x EmbSize x MaxTextLen\n",
        "\n",
        "        if(self.cnn_rnn!=True):\n",
        "          context_features = self.token_cnn(token_features) # BatchSize x EmbSize x MaxTextLen\n",
        "        else:\n",
        "          text_features = self.token_rnn(token_features)   # BatchSize x EmbSize x MaxTextLen\n",
        "        text_features = self.global_pooling_context(context_features).squeeze(-1) # BatchSize x EmbSize\n",
        "        logits = self.classification_head(text_features)   # BatchSize x num_classes\n",
        "        return logits"
      ],
      "metadata": {
        "id": "6YwfGYqtbQDo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Использование LightningDataModule для упрощения работы с нейронной сетью\n",
        "class DataModule(pl.LightningDataModule):\n",
        "  def __init__(self,\n",
        "               tensor_dataset,\n",
        "               batch_size):\n",
        "    super().__init__()\n",
        "    self.tensor_dataset = tensor_dataset\n",
        "    self.batch_size = batch_size\n",
        "    self.prepare_data()\n",
        "\n",
        "  def setup(self, stage=None):\n",
        "    train_size = int(0.8 * len(self.tensor_dataset))\n",
        "    val_size = len(self.tensor_dataset) - train_size\n",
        "    self.train_data, self.val_data = random_split(self.tensor_dataset, [train_size, val_size])\n",
        "    return self.train_data, self.val_data\n",
        "\n",
        "  def train_dataloader(self):\n",
        "    return DataLoader(self.train_data, batch_size=self.batch_size, shuffle=True, num_workers=2)\n",
        "\n",
        "  def val_dataloader(self):\n",
        "    return DataLoader(self.val_data, batch_size=self.batch_size, shuffle = False, num_workers=2)"
      ],
      "metadata": {
        "id": "uB1l60S7e0ML"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Создание модели\n",
        "class ModelCompilation(pl.LightningModule):\n",
        "    def __init__(self,\n",
        "                 model:torch.nn.Module,\n",
        "                 metrics:dict,\n",
        "                 loss_function,\n",
        "                 optimizer:torch.optim,\n",
        "                 learning_rate:float):\n",
        "        super().__init__()\n",
        "        self.model = model\n",
        "        self.metrics = metrics\n",
        "        self.loss_function = loss_function\n",
        "        self.optimizer = optimizer\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "    def forward(self, x):\n",
        "        pred = self.model.forward(x)\n",
        "        return pred\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        train_optimizer = self.optimizer(self.parameters(), lr=self.learning_rate, weight_decay=0.05, betas = (0.9, 0.98), eps = 1.0e-9)\n",
        "        train_scheduler = {\n",
        "            \"scheduler\": ReduceLROnPlateau(optimizer=train_optimizer, mode=\"min\", factor=0.5, patience=3, min_lr=5e-6),\n",
        "            \"interval\": \"epoch\",\n",
        "            \"frequency\": 1,\n",
        "            \"monitor\": \"val_loss\",\n",
        "        }\n",
        "        return [train_optimizer], [train_scheduler]\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        loss, pred, y = self.common_step(batch, batch_idx, 'train')\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        loss, pred, y = self.common_step(batch, batch_idx, 'val')\n",
        "        return loss\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        loss, pred, y = self.common_step(batch, batch_idx, 'test')\n",
        "        return loss\n",
        "\n",
        "    def common_step(self, batch, batch_idx, stage):\n",
        "        x, y = batch\n",
        "        pred = self.forward(x)\n",
        "        loss = self.loss_function(pred, y)\n",
        "        if (stage == 'test') or (stage == 'val'):\n",
        "            on_step = False\n",
        "        else:\n",
        "            on_step = True\n",
        "\n",
        "        [self.log(stage + '_' + metric_name, metric(pred, y).to(device), on_step=on_step, on_epoch=True, prog_bar=True, logger=True) for metric_name, metric in self.metrics.items()]\n",
        "        self.log(stage + '_' + 'loss', loss, on_step=on_step, on_epoch=True, prog_bar=True, logger=True)\n",
        "        return loss, pred, y"
      ],
      "metadata": {
        "id": "4XI-Pd3ElFIF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class History(Callback):\n",
        "    def __init__(self):\n",
        "        self.history = {'val_loss': np.array([]), 'val_accuracy': np.array([]), 'train_loss_epoch' : np.array([]), 'train_accuracy_epoch' : np.array([])}\n",
        "\n",
        "    def on_train_epoch_end(self, trainer, module):\n",
        "        logs = trainer.logged_metrics\n",
        "        self.history['train_loss_epoch'] = np.append(self.history['train_loss_epoch'], logs['train_loss_epoch'].cpu())\n",
        "        self.history['val_loss'] = np.append(self.history['val_loss'], logs['val_loss'].cpu())\n",
        "        self.history['train_accuracy_epoch'] = np.append(self.history['train_accuracy_epoch'], logs['train_accuracy_epoch'].cpu())\n",
        "        self.history['val_accuracy'] = np.append(self.history['val_accuracy'], logs['val_accuracy'].cpu())"
      ],
      "metadata": {
        "id": "9S8zJaDKnT1o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Обучение модели\n",
        "def fit(iteration):\n",
        "  num_classes = 5\n",
        "  task = 'multiclass'\n",
        "  network = Network(cnn_rnn = False,\n",
        "                    vocab_size=vocab_size,\n",
        "                    num_classes=num_classes,\n",
        "                    max_text_len=max_text_len,\n",
        "                    char_embedding_size=64,\n",
        "                    token_embedding_size=256,\n",
        "                    classifier_dropout=0.5\n",
        "                    )\n",
        "\n",
        "  metrics = {'accuracy': torchmetrics.Accuracy(task=task, num_classes=num_classes).to(device)}\n",
        "  loss_function = nn.CrossEntropyLoss()\n",
        "  optimizer = torch.optim.AdamW\n",
        "  learning_rate = 0.0005\n",
        "\n",
        "  earlystopping_callback = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=5)\n",
        "  checkpoint_callback = ModelCheckpoint(monitor='val_loss', filename='model-{epoch:02d}-{val_loss:.2f}-{val_accuracy:.2f}')\n",
        "  history_callback = History()\n",
        "\n",
        "  model = ModelCompilation(network, metrics, loss_function, optimizer, learning_rate)\n",
        "  trainer = pl.Trainer(callbacks=[earlystopping_callback, history_callback, checkpoint_callback], precision='32', accelerator=device, devices=1, max_epochs=50)\n",
        "\n",
        "  trainer.fit(model, datamodule=data_module)\n",
        "  print(\"Best model score for iteration \" + str(iteration) + \" : \" + str(checkpoint_callback.best_model_score))\n",
        "  return checkpoint_callback, history_callback"
      ],
      "metadata": {
        "id": "IUDB0YNRloq4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Обучение моделей заданное количество раз и выбор лучшей модели\n",
        "device = 'cuda'\n",
        "trials = 10\n",
        "best_score = 100\n",
        "best_model_path = ''\n",
        "data_module = DataModule(tensor_dataset=train_dataset, batch_size=8)\n",
        "for i in range(trials):\n",
        "  checkpoint, history = fit(i)\n",
        "  if(checkpoint.best_model_score < best_score):\n",
        "    best_score = checkpoint.best_model_score\n",
        "    best_model_path = checkpoint.best_model_path\n",
        "    best_history = history\n",
        "\n",
        "print(\"Best model score for \" + str(trials) + \" trials: \" + str(best_score))\n",
        "print(\"Best model path: \" + best_model_path)"
      ],
      "metadata": {
        "id": "CbdYcc7BVV1D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_model_path"
      ],
      "metadata": {
        "id": "bxyo1McpCx4I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Вывод графиков\n",
        "def plot_train_metrics(history_callback):\n",
        "  plt.figure()\n",
        "  plt.plot(list(range(len(history_callback.history['train_loss_epoch']))), history_callback.history['train_loss_epoch'])\n",
        "  plt.plot(list(range(len(history_callback.history['val_loss']))), history_callback.history['val_loss'])\n",
        "  plt.legend(['train_loss', 'val_loss'])\n",
        "\n",
        "  plt.figure()\n",
        "  plt.plot(list(range(len(history_callback.history['train_accuracy_epoch']))), history_callback.history['train_accuracy_epoch'])\n",
        "  plt.plot(list(range(len(history_callback.history['val_accuracy']))), history_callback.history['val_accuracy'])\n",
        "  plt.legend(['train_accuracy', 'val_accuracy'])\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "Pxk8Xqzl9wpP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_train_metrics(best_history)"
      ],
      "metadata": {
        "id": "IFIJp9LPVsQF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Загрузка предобученной модели\n",
        "def load_pretrained_model(checkpoint_path):\n",
        "  num_classes = 5\n",
        "  task = 'multiclass'\n",
        "  network = Network(num_classes=num_classes,\n",
        "                    max_text_len=max_text_len,\n",
        "                    token_embedding_size=256,\n",
        "                    classifier_dropout=0.5\n",
        "                    )\n",
        "\n",
        "  metrics = {'accuracy': torchmetrics.Accuracy(task=task, num_classes=num_classes).to(device)}\n",
        "  loss_function = nn.CrossEntropyLoss()\n",
        "  optimizer = torch.optim.AdamW\n",
        "  learning_rate = 0.0005\n",
        "\n",
        "  model = ModelCompilation(network, metrics, loss_function, optimizer, learning_rate)\n",
        "  model = model.load_from_checkpoint(checkpoint_path)\n",
        "  return model"
      ],
      "metadata": {
        "id": "hWEFuMli7hlQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Предсказание стиля текста\n",
        "def predictStyleForText(text, model, vocab_to_int, max_text_len, max_token_len):\n",
        "  texts_tensor = torch.zeros(1, max_text_len, max_token_len + 2).long()\n",
        "  text = text.lower()\n",
        "  segmenter = Segmenter()\n",
        "  doc = Doc(text)\n",
        "  doc.segment(segmenter)\n",
        "  text_tokens = [token.text for token in doc.tokens]\n",
        "  for token_i, token in enumerate(text_tokens):\n",
        "    for char_i, char in enumerate(token):\n",
        "      texts_tensor[0][token_i, char_i + 1] = vocab_to_int[char]\n",
        "\n",
        "  model.eval()\n",
        "  logits = model.forward(texts_tensor)\n",
        "  proba = torch.nn.functional.softmax(logits.data, dim=1)\n",
        "  return proba"
      ],
      "metadata": {
        "id": "IK6tufG7ohG_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = load_pretrained_model(best_model_path)"
      ],
      "metadata": {
        "id": "r-avFiG87rAW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Привет! Здравствуйте, как я могу вам помочь? Я ищу подарок для своей сестры на ее день рождения, что вы могли бы предложить? Намного проще будет, если я знаю интересы вашей сестры. Она занимается каким-то хобби или у нее есть любимый цвет? Она увлекается рисованием и любит всю гамму зеленых цветов. Могу предложить ей набор кистей и атрибутов для рисования, а может быть, замечательный зеленый шарф, который будет сочетаться с любой одеждой. Звучит отлично! Я думаю, она будет в восторге от набора кистей. Отлично, я помогу вам найти то, что вам нужно. Какой предпочитаете ценовой диапазон? Не больше 50 долларов, я хочу не только порадовать сестру, но и оставить себе возможность купить себе что-то вкусное. Понял, буду искать в этом диапазоне. А вы готовы купить сейчас или хотите подумать еще? Я хотел бы сделать покупку сейчас, если вы уверены, что это лучший выбор. Я уверен, что это отличный выбор. Давайте перейдем к кассе, чтобы я мог принять ваш заказ. Спасибо, что выбрали наш магазин!\""
      ],
      "metadata": {
        "id": "lQTBzDTosWHW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictStyleForText(text, model, vocab_to_int, max_text_len, max_token_len)"
      ],
      "metadata": {
        "id": "CXgEb_DkqmSk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}